/*
========================================================================================
    HPC (High Performance Computing) config file
========================================================================================
    Configuration for running on HPC clusters (SLURM, PBS, SGE, etc.)
----------------------------------------------------------------------------------------
*/

params {
    // HPC-specific parameters
    max_memory                 = '128.GB'
    max_cpus                   = 32
    max_time                   = '72.h'
    
    // Queue/partition settings
    queue                      = null
    account                    = null
    clusterOptions             = null
}

process {
    
    // Default executor
    executor = 'slurm'
    
    // Resource scaling for HPC
    withLabel:process_single {
        cpus   = 1
        memory = { check_max( 6.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 4.h  * task.attempt, 'time'    ) }
        queue  = params.queue
    }
    
    withLabel:process_low {
        cpus   = { check_max( 4     * task.attempt, 'cpus'    ) }
        memory = { check_max( 16.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 8.h   * task.attempt, 'time'    ) }
        queue  = params.queue
    }
    
    withLabel:process_medium {
        cpus   = { check_max( 8     * task.attempt, 'cpus'    ) }
        memory = { check_max( 32.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 12.h  * task.attempt, 'time'    ) }
        queue  = params.queue
    }
    
    withLabel:process_high {
        cpus   = { check_max( 16    * task.attempt, 'cpus'    ) }
        memory = { check_max( 64.GB * task.attempt, 'memory'  ) }
        time   = { check_max( 24.h  * task.attempt, 'time'    ) }
        queue  = params.queue
    }
    
    withLabel:process_long {
        time   = { check_max( 48.h  * task.attempt, 'time'    ) }
        queue  = params.queue
    }
    
    withLabel:process_high_memory {
        memory = { check_max( 256.GB * task.attempt, 'memory' ) }
        queue  = params.queue
    }
    
    // Tool-specific HPC settings
    withName:'STAR_.*' {
        cpus   = { check_max( 16    * task.attempt, 'cpus'     ) }
        memory = { check_max( 64.GB * task.attempt, 'memory'   ) }
        time   = { check_max( 12.h  * task.attempt, 'time'     ) }
        queue  = params.queue
    }
    
    withName:'GATK4_.*' {
        cpus   = { check_max( 8     * task.attempt, 'cpus'     ) }
        memory = { check_max( 32.GB * task.attempt, 'memory'   ) }
        time   = { check_max( 24.h  * task.attempt, 'time'     ) }
        queue  = params.queue
    }
    
    withName:SALMON_INDEX {
        cpus   = { check_max( 8     * task.attempt, 'cpus'     ) }
        memory = { check_max( 16.GB * task.attempt, 'memory'   ) }
        time   = { check_max( 4.h   * task.attempt, 'time'     ) }
        queue  = params.queue
    }
    
    // SLURM-specific settings
    clusterOptions = {
        def options = []
        
        if (params.account) {
            options << "--account=${params.account}"
        }
        
        if (params.clusterOptions) {
            options << params.clusterOptions
        }
        
        // Add any additional SLURM options
        options << "--export=ALL"
        options << "--exclusive"  // For full node usage
        
        return options.join(' ')
    }()
}

// Container settings for HPC
singularity {
    enabled    = true
    autoMounts = true
    cacheDir   = "/scratch/${USER}/.singularity"
    libraryDir = "/scratch/${USER}/.singularity"
}

// Disable other container engines
docker.enabled      = false
podman.enabled      = false
shifter.enabled     = false
charliecloud.enabled = false

// Work directory for large temporary files
workDir = "/scratch/${USER}/nextflow-work"

// Conda settings (alternative to containers)
conda {
    enabled = false  // Prefer containers on HPC
    cacheDir = "/scratch/${USER}/.conda"
}

// Executor-specific settings
executor {
    $slurm {
        // SLURM configuration
        queueSize = 50
        submitRateLimit = '10 sec'
        pollInterval = '30 sec'
        queueStatInterval = '5 min'
    }
    
    $pbs {
        // PBS configuration
        queueSize = 50
        submitRateLimit = '10 sec'
        pollInterval = '30 sec'
    }
    
    $sge {
        // SGE configuration
        queueSize = 50
        submitRateLimit = '10 sec'
        pollInterval = '30 sec'
    }
}

